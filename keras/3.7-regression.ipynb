{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, set environment variables and initialize spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env SPARK_DRIVER_MEMORY=8g\n",
    "%env PYSPARK_PYTHON=/usr/bin/python3.5\n",
    "%env PYSPARK_DRIVER_PYTHON=/usr/bin/python3.5\n",
    "\n",
    "from zoo.common.nncontext import *\n",
    "sc = init_nncontext(init_spark_conf().setMaster(\"local[4]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is packaged in Keras 2.0.8 but not in Keras 1.2.2, so that we need to use following code to get the data, then we also apply normalization on these data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "def load_data(path='boston_housing.npz', test_split=0.2, seed=113):\n",
    "    \"\"\"Loads the Boston Housing dataset.\n",
    "    # Arguments\n",
    "        path: path where to cache the dataset locally\n",
    "            (relative to ~/.zoo.pipeline.api.keras/datasets).\n",
    "        test_split: fraction of the data to reserve as test set.\n",
    "        seed: Random seed for shuffling the data\n",
    "            before computing the test split.\n",
    "    # Returns\n",
    "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "    \"\"\"\n",
    "    assert 0 <= test_split < 1\n",
    "    path = get_file(\n",
    "        path,\n",
    "        origin='https://s3.amazonaws.com/zoo.pipeline.api.keras-datasets/boston_housing.npz'\n",
    "        )\n",
    "    with np.load(path) as f:\n",
    "        x = f['x']\n",
    "        y = f['y']\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(x))\n",
    "    np.random.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    x_train = np.array(x[:int(len(x) * (1 - test_split))])\n",
    "    y_train = np.array(y[:int(len(x) * (1 - test_split))])\n",
    "    x_test = np.array(x[int(len(x) * (1 - test_split)):])\n",
    "    y_test = np.array(y[int(len(x) * (1 - test_split)):])\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = load_data()\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_nb_epoch = 50\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    #model.fit(partial_train_data, partial_train_targets,\n",
    "    #          nb_epoch=num_nb_epoch, batch_size=1, verbose=0)\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "              nb_epoch=num_nb_epoch, batch_size=16)\n",
    "\n",
    "    # Evaluate the model on the validation data\n",
    "    #val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    val_mae = model.evaluate(val_data, val_targets)\n",
    "    all_scores.append(val_mae[0].result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "# Train it on the entirety of the data.\n",
    "model.fit(train_data, train_targets,\n",
    "          nb_epoch=80, batch_size=16)\n",
    "test_result = model.evaluate(test_data, test_targets)\n",
    "\n",
    "print('test result:', test_result[0].result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
