{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, set environment variables and initialize spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env SPARK_DRIVER_MEMORY=8g\n",
    "%env PYSPARK_PYTHON=/usr/bin/python3.5\n",
    "%env PYSPARK_DRIVER_PYTHON=/usr/bin/python3.5\n",
    "\n",
    "from zoo.common.nncontext import *\n",
    "sc = init_nncontext(init_spark_conf().setMaster(\"local[4]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "We will be attempting to predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points about the suburb at the time, such as the crime rate, the local property tax rate, etc.\n",
    "\n",
    "The dataset we will be using has another interesting difference from our two previous examples: it has very few data points, only 506 in total, split between 404 training samples and 102 test samples, and each \"feature\" in the input data (e.g. the crime rate is a feature) has a different scale. For instance some values are proportions, which take a values between 0 and 1, others take values between 1 and 12, others between 0 and 100..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is packaged in Keras 2.0.8 but not in Keras 1.2.2, so that we need to use following code to get the data, then we also apply normalization on these data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "def load_data(path='boston_housing.npz', test_split=0.2, seed=113):\n",
    "    \"\"\"Loads the Boston Housing dataset.\n",
    "    # Arguments\n",
    "        path: path where to cache the dataset locally\n",
    "            (relative to ~/.zoo.pipeline.api.keras/datasets).\n",
    "        test_split: fraction of the data to reserve as test set.\n",
    "        seed: Random seed for shuffling the data\n",
    "            before computing the test split.\n",
    "    # Returns\n",
    "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "    \"\"\"\n",
    "    assert 0 <= test_split < 1\n",
    "    path = get_file(\n",
    "        path,\n",
    "        origin='https://s3.amazonaws.com/zoo.pipeline.api.keras-datasets/boston_housing.npz'\n",
    "        )\n",
    "    with np.load(path) as f:\n",
    "        x = f['x']\n",
    "        y = f['y']\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(x))\n",
    "    np.random.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    x_train = np.array(x[:int(len(x) * (1 - test_split))])\n",
    "    y_train = np.array(y[:int(len(x) * (1 - test_split))])\n",
    "    x_test = np.array(x[int(len(x) * (1 - test_split)):])\n",
    "    y_test = np.array(y[int(len(x) * (1 - test_split)):])\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = load_data()\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we have so few data points, the validation set would end up being very small (e.g. about 100 examples). A consequence is that our validation scores may change a lot depending on which data points we choose to use for validation and which we choose for training, i.e. the validation scores may have a high variance with regard to the validation split. This would prevent us from reliably evaluating our model.\n",
    "\n",
    "The best practice in such situations is to use K-fold cross-validation. It consists of splitting the available data into K partitions (typically K=4 or 5), then instantiating K identical models, and training each one on K-1 partitions while evaluating on the remaining partition. The validation score for the model used would then be the average of the K validation scores obtained.\n",
    "\n",
    "Since we are using K-fold so that we have to build the model multiple times, we use following function to build our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.pipeline.api.keras import models\n",
    "from zoo.pipeline.api.keras import layers\n",
    "\n",
    "def build_model():\n",
    "    # Because we will need to instantiate\n",
    "    # the same model multiple times,\n",
    "    # we use a function to construct it.\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "                           input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's start our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_nb_epoch = 50\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    #model.fit(partial_train_data, partial_train_targets,\n",
    "    #          nb_epoch=num_nb_epoch, batch_size=1, verbose=0)\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "              nb_epoch=num_nb_epoch, batch_size=16)\n",
    "\n",
    "    # Evaluate the model on the validation data\n",
    "    #val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    val_mae = model.evaluate(val_data, val_targets)\n",
    "    all_scores.append(val_mae[0].result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing fold # 0\n",
    "Trained 16 records in 0.011235845 seconds. Throughput is 1424.0139 records/second. Loss is 8.708786.\n",
    "processing fold # 1\n",
    "Trained 16 records in 0.009535034 seconds. Throughput is 1678.0223 records/second. Loss is 5.3613434.\n",
    "processing fold # 2\n",
    "Trained 16 records in 0.008636178 seconds. Throughput is 1852.6713 records/second. Loss is 18.106756.\n",
    "processing fold # 3\n",
    "Trained 16 records in 0.009207628 seconds. Throughput is 1737.6897 records/second. Loss is 7.0931993."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could check our K-fold training result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[20.572654724121094, 19.606250762939453, 21.224998474121094, 22.60078239440918]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
